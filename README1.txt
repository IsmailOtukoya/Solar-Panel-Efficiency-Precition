#  Solar Panel Efficiency Prediction (Optimized Ensemble)

This notebook was developed for a hackathon challenge focused on predicting solar panel efficiency based on various environmental and operational parameters. The main goal was to build a highly accurate model using ensemble learning techniques while also optimizing hyperparameters for better performance.

## Project Objective

To predict the output efficiency of solar panels using environmental and operational data. I aimed to combine the strengths of gradient boosting models using a stacked ensemble approach and optimize them for accuracy.

## Tools & Libraries Used

- `pandas`, `numpy` for data wrangling and numerical operations
- `scikit-learn` for preprocessing, model evaluation, and stacking
- `xgboost`, `lightgbm` for building high-performance models
- `optuna` for hyperparameter optimization
- `warnings`, `LabelEncoder`, etc. for utility support

##  Approach

1. **Data Cleaning & Preprocessing**
   - Defined a preprocessing function to handle missing values, encode categorical data, and scale features if needed.
   - Focused on numeric features such as temperature, irradiance, panel age, voltage, current, humidity, etc.

2. **Modeling**
   - Used **XGBoost** and **LightGBM** as base models.
   - Performed hyperparameter tuning with **Optuna** to get the best parameters.
   - Trained both models using 5-fold cross-validation.

3. **Stacking**
   - Combined the predictions of the two models using a **RidgeCV** meta-model.
   - Final predictions were generated by averaging test predictions and passing them to the stacked model.

##  Feature Engineering

  - `temp_diff` = module_temperature - temperature
   - `irradiance_per_cloud` = irradiance / (cloud_coverage + 1)
   - `maintenance_rate` = maintenance_count / (panel_age + 1)
   - `voltage_norm` and `current_norm` = voltage/current normalized by irradiance

Key features used:
- `temperature`
- `irradiance`
- `panel_age`
- `maintenance_count`
- `soiling_ratio`
- `voltage`, `current`
- `cloud_coverage`
- `module_temperature`
- `humidity`

I ended up using all the features due to improved accuracy when all features were used. But the features above were the features that contributed the most based on feature importance

##  Optimization Strategy

- **Optuna** was used to tune `n_estimators`, `learning_rate`, and `max_depth` among other parameters for both LightGBM and XGBoost.
- Goal was to minimize RMSE over cross-validation.

##  Files

- `Solar_Optimized_Ensemble.ipynb`: Original notebook with full optimization and modeling.


##  How to Run

1. Install dependencies from requirements.txt :
   ```bash
   pip install pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm optuna
   ```

2. Run the notebook from top to bottom. It is self-contained and assumes data is loaded within the script.

---

